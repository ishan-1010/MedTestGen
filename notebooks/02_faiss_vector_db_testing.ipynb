{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# FAISS Vector Database Testing for Healthcare Test Case Generation\n",
        "\n",
        "This notebook demonstrates the use of FAISS (Facebook AI Similarity Search) for building a vector database to store and retrieve healthcare test cases. This is foundational for the AI-powered test case generation system as outlined in the NASSCOM problem statement.\n",
        "\n",
        "## Key Objectives:\n",
        "1. Create sample healthcare test cases and requirements\n",
        "2. Convert text to embeddings using Sentence Transformers\n",
        "3. Build and test a FAISS vector index\n",
        "4. Perform similarity searches to find relevant test cases\n",
        "5. Demonstrate how this can support automated test case generation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n",
            "FAISS version: 1.12.0\n",
            "NumPy version: 2.3.3\n",
            "Pandas version: 2.3.2\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import faiss\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import json\n",
        "from typing import List, Dict, Any\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "print(f\"FAISS version: {faiss.__version__ if hasattr(faiss, '__version__') else 'Unknown'}\")\n",
        "print(f\"NumPy version: {np.__version__}\")\n",
        "print(f\"Pandas version: {pd.__version__}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Initialize Embedding Model\n",
        "\n",
        "We'll use Sentence Transformers to convert text into embeddings. The model `all-MiniLM-L6-v2` is lightweight yet effective for semantic similarity tasks.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c9ef102e906e403391e885a36902bd2d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdc5eeb96863457daab0973eafba44fb",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "137ccface9b44d6a97b3a6d708e52e95",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "48db0abbe4a44c60be14662c9efd19e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "71270b27ee374238bd13af755f91b191",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d469988336c149c29518a067e6884389",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a9cd4f02d8f4409bb06b7e58c8603268",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef5fad7c4810480cb9592a6ed67e083d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ec9a82bd6314abba7fcae8b044b53fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "36fbfd060e944d018dd5ab418decacb4",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1ab72c640fd74104ace0dfc323a4d795",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model loaded successfully!\n",
            "Embedding dimension: 384\n",
            "Sample embedding (first 10 values): [-0.09741534  0.10009975 -0.05815928 -0.0040833   0.02157647 -0.00060688\n",
            "  0.04464454 -0.01896917 -0.00650744 -0.05049402]\n"
          ]
        }
      ],
      "source": [
        "# Initialize the sentence transformer model\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "# Test the model with a sample healthcare text\n",
        "sample_text = \"Patient blood pressure monitoring test case\"\n",
        "sample_embedding = model.encode(sample_text)\n",
        "\n",
        "print(f\"‚úÖ Model loaded successfully!\")\n",
        "print(f\"Embedding dimension: {len(sample_embedding)}\")\n",
        "print(f\"Sample embedding (first 10 values): {sample_embedding[:10]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Create Sample Healthcare Test Cases\n",
        "\n",
        "Let's create a diverse set of healthcare test cases that cover various aspects of medical software testing, including:\n",
        "- Patient data management\n",
        "- Medical device integration\n",
        "- Clinical workflows\n",
        "- Compliance requirements\n",
        "- Emergency scenarios\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Created 15 healthcare test cases\n",
            "\n",
            "Categories: ['Patient Management' 'Medical Device Integration'\n",
            " 'Clinical Decision Support' 'Interoperability' 'Clinical Workflow'\n",
            " 'Compliance' 'Imaging' 'Pharmacy' 'Telehealth' 'Research'\n",
            " 'Patient Engagement' 'Immunization']\n",
            "\n",
            "Priorities: {'High': 7, 'Critical': 6, 'Medium': 2}\n",
            "\n",
            "Sample test case:\n",
            "{'id': 'TC001', 'title': 'Patient Registration with Complete Demographics', 'category': 'Patient Management'}\n"
          ]
        }
      ],
      "source": [
        "# Create sample healthcare test cases\n",
        "healthcare_test_cases = [\n",
        "    {\n",
        "        \"id\": \"TC001\",\n",
        "        \"title\": \"Patient Registration with Complete Demographics\",\n",
        "        \"description\": \"Verify that a new patient can be registered with all required demographic information including name, DOB, address, insurance, and emergency contact\",\n",
        "        \"category\": \"Patient Management\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"HIPAA\", \"GDPR\"],\n",
        "        \"test_steps\": \"1. Navigate to patient registration\\n2. Enter all required fields\\n3. Validate insurance\\n4. Save patient record\",\n",
        "        \"expected_result\": \"Patient record created successfully with unique MRN assigned\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC002\",\n",
        "        \"title\": \"Blood Pressure Monitor Integration Test\",\n",
        "        \"description\": \"Test integration with Omron blood pressure monitor device for automatic vital signs capture\",\n",
        "        \"category\": \"Medical Device Integration\",\n",
        "        \"priority\": \"Critical\",\n",
        "        \"compliance\": [\"FDA 510(k)\", \"ISO 13485\"],\n",
        "        \"test_steps\": \"1. Connect BP monitor via Bluetooth\\n2. Take patient reading\\n3. Verify automatic data transfer\\n4. Check data accuracy\",\n",
        "        \"expected_result\": \"BP readings automatically transferred and stored in patient chart within 5 seconds\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC003\",\n",
        "        \"title\": \"Medication Allergy Alert System\",\n",
        "        \"description\": \"Verify that system generates appropriate alerts when prescribing medications that patient is allergic to\",\n",
        "        \"category\": \"Clinical Decision Support\",\n",
        "        \"priority\": \"Critical\",\n",
        "        \"compliance\": [\"FDA\", \"ISO 27001\"],\n",
        "        \"test_steps\": \"1. Add penicillin allergy to patient record\\n2. Attempt to prescribe amoxicillin\\n3. Verify alert appears\\n4. Test override with reason\",\n",
        "        \"expected_result\": \"System displays critical allergy alert and requires documented reason for override\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC004\",\n",
        "        \"title\": \"HL7 Lab Results Import\",\n",
        "        \"description\": \"Test importing laboratory results via HL7 v2.5 message format from external lab system\",\n",
        "        \"category\": \"Interoperability\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"HL7 Standards\", \"CLIA\"],\n",
        "        \"test_steps\": \"1. Send HL7 ORU message\\n2. Verify message parsing\\n3. Check result mapping\\n4. Confirm physician notification\",\n",
        "        \"expected_result\": \"Lab results correctly parsed, stored, and physician notified of critical values\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC005\",\n",
        "        \"title\": \"Emergency Department Triage Workflow\",\n",
        "        \"description\": \"Test complete ED triage workflow including ESI level assignment and vital signs documentation\",\n",
        "        \"category\": \"Clinical Workflow\",\n",
        "        \"priority\": \"Critical\",\n",
        "        \"compliance\": [\"EMTALA\", \"Joint Commission\"],\n",
        "        \"test_steps\": \"1. Register walk-in patient\\n2. Complete triage assessment\\n3. Assign ESI level\\n4. Document vital signs\\n5. Queue for provider\",\n",
        "        \"expected_result\": \"Patient triaged within 10 minutes with appropriate ESI level and queued correctly\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC006\",\n",
        "        \"title\": \"GDPR Data Export Request\",\n",
        "        \"description\": \"Verify system can export all patient data in machine-readable format for GDPR compliance\",\n",
        "        \"category\": \"Compliance\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"GDPR\", \"Data Protection\"],\n",
        "        \"test_steps\": \"1. Submit data export request\\n2. Verify all data categories included\\n3. Check export format\\n4. Validate completeness\",\n",
        "        \"expected_result\": \"Complete patient data exported in JSON/XML format within 30 days\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC007\",\n",
        "        \"title\": \"Radiology PACS Image Viewer\",\n",
        "        \"description\": \"Test viewing and manipulating DICOM images from PACS system\",\n",
        "        \"category\": \"Imaging\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"DICOM\", \"FDA\"],\n",
        "        \"test_steps\": \"1. Open patient imaging study\\n2. Test zoom/pan/window level\\n3. Add measurements\\n4. Create report\",\n",
        "        \"expected_result\": \"Images load within 3 seconds with full manipulation capabilities\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC008\",\n",
        "        \"title\": \"Prescription Drug Monitoring Integration\",\n",
        "        \"description\": \"Verify integration with state PDMP for controlled substance monitoring\",\n",
        "        \"category\": \"Pharmacy\",\n",
        "        \"priority\": \"Critical\",\n",
        "        \"compliance\": [\"DEA\", \"State Regulations\"],\n",
        "        \"test_steps\": \"1. Prescribe controlled substance\\n2. Query PDMP database\\n3. Review patient history\\n4. Document review\",\n",
        "        \"expected_result\": \"PDMP query completed and documented before prescription finalized\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC009\",\n",
        "        \"title\": \"Telemedicine Video Consultation\",\n",
        "        \"description\": \"Test end-to-end telemedicine visit including video, audio, and documentation\",\n",
        "        \"category\": \"Telehealth\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"HIPAA\", \"State Licensure\"],\n",
        "        \"test_steps\": \"1. Schedule virtual visit\\n2. Test video/audio quality\\n3. Complete consultation\\n4. Generate visit summary\",\n",
        "        \"expected_result\": \"Successful encrypted video consultation with complete documentation\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC010\",\n",
        "        \"title\": \"Clinical Trial Enrollment Screening\",\n",
        "        \"description\": \"Test automated screening for clinical trial eligibility based on patient criteria\",\n",
        "        \"category\": \"Research\",\n",
        "        \"priority\": \"Medium\",\n",
        "        \"compliance\": [\"GCP\", \"21 CFR Part 11\"],\n",
        "        \"test_steps\": \"1. Define trial criteria\\n2. Run eligibility query\\n3. Review matched patients\\n4. Generate recruitment list\",\n",
        "        \"expected_result\": \"System identifies all eligible patients meeting inclusion/exclusion criteria\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC011\",\n",
        "        \"title\": \"Insulin Pump Data Integration\",\n",
        "        \"description\": \"Test integration with Medtronic insulin pump for diabetes management\",\n",
        "        \"category\": \"Medical Device Integration\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"FDA Class II\", \"ISO 13485\"],\n",
        "        \"test_steps\": \"1. Connect insulin pump\\n2. Download pump data\\n3. Analyze glucose patterns\\n4. Generate recommendations\",\n",
        "        \"expected_result\": \"Pump data successfully imported with trend analysis and recommendations\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC012\",\n",
        "        \"title\": \"Audit Trail for Controlled Substances\",\n",
        "        \"description\": \"Verify complete audit trail for all controlled substance prescriptions\",\n",
        "        \"category\": \"Compliance\",\n",
        "        \"priority\": \"Critical\",\n",
        "        \"compliance\": [\"DEA\", \"21 CFR Part 11\"],\n",
        "        \"test_steps\": \"1. Prescribe controlled substance\\n2. Modify prescription\\n3. Cancel prescription\\n4. Review audit log\",\n",
        "        \"expected_result\": \"All actions logged with timestamp, user, and reason for change\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC013\",\n",
        "        \"title\": \"Sepsis Alert Algorithm\",\n",
        "        \"description\": \"Test early warning system for sepsis detection based on vital signs and lab values\",\n",
        "        \"category\": \"Clinical Decision Support\",\n",
        "        \"priority\": \"Critical\",\n",
        "        \"compliance\": [\"CMS SEP-1\", \"Clinical Guidelines\"],\n",
        "        \"test_steps\": \"1. Enter abnormal vitals\\n2. Add lab results\\n3. Verify alert triggers\\n4. Test response workflow\",\n",
        "        \"expected_result\": \"Sepsis alert triggered within 1 hour of meeting criteria with bundled orders\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC014\",\n",
        "        \"title\": \"Patient Portal Secure Messaging\",\n",
        "        \"description\": \"Test secure messaging between patients and providers through patient portal\",\n",
        "        \"category\": \"Patient Engagement\",\n",
        "        \"priority\": \"Medium\",\n",
        "        \"compliance\": [\"HIPAA\", \"Meaningful Use\"],\n",
        "        \"test_steps\": \"1. Patient sends message\\n2. Provider receives notification\\n3. Provider responds\\n4. Check encryption\",\n",
        "        \"expected_result\": \"Messages encrypted end-to-end with delivery confirmation\"\n",
        "    },\n",
        "    {\n",
        "        \"id\": \"TC015\",\n",
        "        \"title\": \"Vaccine Administration Record\",\n",
        "        \"description\": \"Test complete vaccine administration workflow including inventory and reporting\",\n",
        "        \"category\": \"Immunization\",\n",
        "        \"priority\": \"High\",\n",
        "        \"compliance\": [\"CDC\", \"State Registry\"],\n",
        "        \"test_steps\": \"1. Check vaccine inventory\\n2. Document administration\\n3. Update immunization record\\n4. Report to state registry\",\n",
        "        \"expected_result\": \"Vaccine documented and reported to state registry within 24 hours\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Convert to DataFrame for easier manipulation\n",
        "df_test_cases = pd.DataFrame(healthcare_test_cases)\n",
        "print(f\"‚úÖ Created {len(df_test_cases)} healthcare test cases\")\n",
        "print(f\"\\nCategories: {df_test_cases['category'].unique()}\")\n",
        "print(f\"\\nPriorities: {df_test_cases['priority'].value_counts().to_dict()}\")\n",
        "print(f\"\\nSample test case:\\n{df_test_cases.iloc[0][['id', 'title', 'category']].to_dict()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Generate Embeddings for Test Cases\n",
        "\n",
        "We'll create embeddings by combining multiple fields to capture the full context of each test case.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generating embeddings for all test cases...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ae0b324507ae49c489506b3e6ab3cbcf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Generated embeddings with shape: (15, 384)\n",
            "Embedding dimension: 384\n"
          ]
        }
      ],
      "source": [
        "# Create combined text for each test case for better embedding quality\n",
        "def create_test_case_text(row):\n",
        "    \"\"\"Combine relevant fields to create comprehensive text representation\"\"\"\n",
        "    compliance_str = ', '.join(row['compliance']) if isinstance(row['compliance'], list) else row['compliance']\n",
        "    \n",
        "    text = f\"\"\"\n",
        "    Test Case: {row['title']}\n",
        "    Category: {row['category']}\n",
        "    Priority: {row['priority']}\n",
        "    Description: {row['description']}\n",
        "    Test Steps: {row['test_steps']}\n",
        "    Expected Result: {row['expected_result']}\n",
        "    Compliance: {compliance_str}\n",
        "    \"\"\"\n",
        "    return text.strip()\n",
        "\n",
        "# Generate combined text for all test cases\n",
        "df_test_cases['combined_text'] = df_test_cases.apply(create_test_case_text, axis=1)\n",
        "\n",
        "# Generate embeddings for all test cases\n",
        "print(\"Generating embeddings for all test cases...\")\n",
        "embeddings = model.encode(df_test_cases['combined_text'].tolist(), show_progress_bar=True)\n",
        "\n",
        "print(f\"‚úÖ Generated embeddings with shape: {embeddings.shape}\")\n",
        "print(f\"Embedding dimension: {embeddings.shape[1]}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Build FAISS Index\n",
        "\n",
        "We'll create different types of FAISS indices and compare their performance:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Flat Index created with 15 vectors\n",
            "‚úÖ IVF Index created with 15 vectors\n",
            "‚úÖ HNSW Index created with 15 vectors\n",
            "\n",
            "üìä Index Comparison:\n",
            "  - Flat Index: Exact search, O(n) complexity\n",
            "  - IVF Index: Approximate search, faster for large datasets\n",
            "  - HNSW Index: Graph-based search, good balance\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING clustering 15 points to 5 centroids: please provide at least 195 training points\n"
          ]
        }
      ],
      "source": [
        "# Build FAISS indices\n",
        "\n",
        "# 1. Flat Index (Exact Search) - Most accurate but slower for large datasets\n",
        "dimension = embeddings.shape[1]\n",
        "index_flat = faiss.IndexFlatL2(dimension)\n",
        "index_flat.add(embeddings.astype('float32'))\n",
        "\n",
        "print(f\"‚úÖ Flat Index created with {index_flat.ntotal} vectors\")\n",
        "\n",
        "# 2. IVF Index (Inverted File Index) - Faster search with slight accuracy tradeoff\n",
        "nlist = 5  # Number of clusters (small dataset, so few clusters)\n",
        "quantizer = faiss.IndexFlatL2(dimension)\n",
        "index_ivf = faiss.IndexIVFFlat(quantizer, dimension, nlist)\n",
        "\n",
        "# Train the index with our data\n",
        "index_ivf.train(embeddings.astype('float32'))\n",
        "index_ivf.add(embeddings.astype('float32'))\n",
        "\n",
        "print(f\"‚úÖ IVF Index created with {index_ivf.ntotal} vectors\")\n",
        "\n",
        "# 3. HNSW Index (Hierarchical Navigable Small World) - Good balance of speed and accuracy\n",
        "index_hnsw = faiss.IndexHNSWFlat(dimension, 32)  # 32 is the connectivity parameter\n",
        "index_hnsw.add(embeddings.astype('float32'))\n",
        "\n",
        "print(f\"‚úÖ HNSW Index created with {index_hnsw.ntotal} vectors\")\n",
        "\n",
        "print(f\"\\nüìä Index Comparison:\")\n",
        "print(f\"  - Flat Index: Exact search, O(n) complexity\")\n",
        "print(f\"  - IVF Index: Approximate search, faster for large datasets\")\n",
        "print(f\"  - HNSW Index: Graph-based search, good balance\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Create Search Function\n",
        "\n",
        "Let's create a reusable search function that can find similar test cases based on queries:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîç Searching for: 'Test blood pressure monitoring device integration with Bluetooth connectivity'\\n\n",
            "Top 3 similar test cases:\n",
            " rank    id                                   title                   category priority  similarity_score\n",
            "    1 TC002 Blood Pressure Monitor Integration Test Medical Device Integration Critical          0.587196\n",
            "    2 TC011           Insulin Pump Data Integration Medical Device Integration     High          0.425134\n",
            "    3 TC004                  HL7 Lab Results Import           Interoperability     High          0.400124\n"
          ]
        }
      ],
      "source": [
        "def search_similar_test_cases(query: str, index: faiss.Index, k: int = 5, df: pd.DataFrame = df_test_cases):\n",
        "    \"\"\"\n",
        "    Search for similar test cases using FAISS\n",
        "    \n",
        "    Args:\n",
        "        query: Search query text\n",
        "        index: FAISS index to search\n",
        "        k: Number of results to return\n",
        "        df: DataFrame containing test case data\n",
        "    \n",
        "    Returns:\n",
        "        DataFrame with top k similar test cases and their similarity scores\n",
        "    \"\"\"\n",
        "    # Generate embedding for the query\n",
        "    query_embedding = model.encode([query])\n",
        "    \n",
        "    # Search the index\n",
        "    distances, indices = index.search(query_embedding.astype('float32'), k)\n",
        "    \n",
        "    # Create results DataFrame\n",
        "    results = []\n",
        "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0])):\n",
        "        if idx != -1:  # Valid result\n",
        "            result = df.iloc[idx].copy()\n",
        "            result['similarity_score'] = 1 / (1 + dist)  # Convert distance to similarity\n",
        "            result['rank'] = i + 1\n",
        "            results.append(result)\n",
        "    \n",
        "    results_df = pd.DataFrame(results)\n",
        "    \n",
        "    # Select relevant columns for display\n",
        "    if len(results_df) > 0:\n",
        "        display_columns = ['rank', 'id', 'title', 'category', 'priority', 'similarity_score']\n",
        "        return results_df[display_columns]\n",
        "    else:\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Test the search function\n",
        "test_query = \"Test blood pressure monitoring device integration with Bluetooth connectivity\"\n",
        "print(f\"üîç Searching for: '{test_query}'\\\\n\")\n",
        "\n",
        "results = search_similar_test_cases(test_query, index_flat, k=3)\n",
        "print(\"Top 3 similar test cases:\")\n",
        "print(results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Test Different Query Scenarios\n",
        "\n",
        "Let's test the system with various healthcare-specific queries to see how well it retrieves relevant test cases:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TESTING DIFFERENT QUERY SCENARIOS\n",
            "================================================================================\n",
            "\n",
            "üìã Testing compliance-related search\n",
            "Query: 'HIPAA compliance for patient data security'\n",
            "Expected Category: Compliance\n",
            "----------------------------------------\n",
            "‚ùå Rank 1: Patient Registration with Complete Demographics...\n",
            "   Category: Patient Management, Score: 0.502\n",
            "‚ùå Rank 2: Patient Portal Secure Messaging...\n",
            "   Category: Patient Engagement, Score: 0.475\n",
            "‚úÖ Rank 3: GDPR Data Export Request...\n",
            "   Category: Compliance, Score: 0.446\n",
            "\n",
            "\n",
            "üìã Testing workflow-related search\n",
            "Query: 'Emergency room critical patient triage workflow'\n",
            "Expected Category: Clinical Workflow\n",
            "----------------------------------------\n",
            "‚úÖ Rank 1: Emergency Department Triage Workflow...\n",
            "   Category: Clinical Workflow, Score: 0.676\n",
            "‚ùå Rank 2: Sepsis Alert Algorithm...\n",
            "   Category: Clinical Decision Support, Score: 0.504\n",
            "‚ùå Rank 3: Patient Portal Secure Messaging...\n",
            "   Category: Patient Engagement, Score: 0.468\n",
            "\n",
            "\n",
            "üìã Testing device integration search\n",
            "Query: 'Integration with medical devices like insulin pumps and glucose monitors'\n",
            "Expected Category: Medical Device Integration\n",
            "----------------------------------------\n",
            "‚úÖ Rank 1: Insulin Pump Data Integration...\n",
            "   Category: Medical Device Integration, Score: 0.577\n",
            "‚úÖ Rank 2: Blood Pressure Monitor Integration Test...\n",
            "   Category: Medical Device Integration, Score: 0.488\n",
            "‚ùå Rank 3: Prescription Drug Monitoring Integration...\n",
            "   Category: Pharmacy, Score: 0.423\n",
            "\n",
            "\n",
            "üìã Testing clinical decision support search\n",
            "Query: 'Alert system for drug interactions and allergies'\n",
            "Expected Category: Clinical Decision Support\n",
            "----------------------------------------\n",
            "‚úÖ Rank 1: Medication Allergy Alert System...\n",
            "   Category: Clinical Decision Support, Score: 0.533\n",
            "‚úÖ Rank 2: Sepsis Alert Algorithm...\n",
            "   Category: Clinical Decision Support, Score: 0.456\n",
            "‚ùå Rank 3: Prescription Drug Monitoring Integration...\n",
            "   Category: Pharmacy, Score: 0.453\n",
            "\n",
            "\n",
            "üìã Testing telehealth-related search\n",
            "Query: 'Telehealth video consultation with encrypted communication'\n",
            "Expected Category: Telehealth\n",
            "----------------------------------------\n",
            "‚úÖ Rank 1: Telemedicine Video Consultation...\n",
            "   Category: Telehealth, Score: 0.646\n",
            "‚ùå Rank 2: Patient Portal Secure Messaging...\n",
            "   Category: Patient Engagement, Score: 0.453\n",
            "‚ùå Rank 3: Emergency Department Triage Workflow...\n",
            "   Category: Clinical Workflow, Score: 0.386\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Test various query scenarios\n",
        "test_queries = [\n",
        "    {\n",
        "        \"query\": \"HIPAA compliance for patient data security\",\n",
        "        \"expected_category\": \"Compliance\",\n",
        "        \"description\": \"Testing compliance-related search\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Emergency room critical patient triage workflow\",\n",
        "        \"expected_category\": \"Clinical Workflow\",\n",
        "        \"description\": \"Testing workflow-related search\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Integration with medical devices like insulin pumps and glucose monitors\",\n",
        "        \"expected_category\": \"Medical Device Integration\",\n",
        "        \"description\": \"Testing device integration search\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Alert system for drug interactions and allergies\",\n",
        "        \"expected_category\": \"Clinical Decision Support\",\n",
        "        \"description\": \"Testing clinical decision support search\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Telehealth video consultation with encrypted communication\",\n",
        "        \"expected_category\": \"Telehealth\",\n",
        "        \"description\": \"Testing telehealth-related search\"\n",
        "    }\n",
        "]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TESTING DIFFERENT QUERY SCENARIOS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "for test_case in test_queries:\n",
        "    print(f\"\\nüìã {test_case['description']}\")\n",
        "    print(f\"Query: '{test_case['query']}'\")\n",
        "    print(f\"Expected Category: {test_case['expected_category']}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    results = search_similar_test_cases(test_case['query'], index_flat, k=3)\n",
        "    \n",
        "    if len(results) > 0:\n",
        "        for _, row in results.iterrows():\n",
        "            match_indicator = \"‚úÖ\" if row['category'] == test_case['expected_category'] else \"‚ùå\"\n",
        "            print(f\"{match_indicator} Rank {row['rank']}: {row['title'][:50]}...\")\n",
        "            print(f\"   Category: {row['category']}, Score: {row['similarity_score']:.3f}\")\n",
        "    else:\n",
        "        print(\"No results found\")\n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 7: Performance Comparison\n",
        "\n",
        "Let's compare the performance of different FAISS index types:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üèÉ Performance Benchmarking Results\n",
            "============================================================\n",
            "\n",
            "Flat (L2) Index:\n",
            "  Average search time: 0.03 ms\n",
            "  Min/Max: 0.03 ms / 0.03 ms\n",
            "  Std deviation: 0.00 ms\n",
            "\n",
            "IVF Index:\n",
            "  Average search time: 0.05 ms\n",
            "  Min/Max: 0.04 ms / 0.08 ms\n",
            "  Std deviation: 0.02 ms\n",
            "\n",
            "HNSW Index:\n",
            "  Average search time: 0.05 ms\n",
            "  Min/Max: 0.04 ms / 0.09 ms\n",
            "  Std deviation: 0.02 ms\n",
            "\n",
            "üìä Summary Comparison:\n",
            "           avg_time  min_time  max_time  std_time\n",
            "Flat (L2)      0.03      0.03      0.03      0.00\n",
            "IVF            0.05      0.04      0.08      0.02\n",
            "HNSW           0.05      0.04      0.09      0.02\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "def benchmark_index(index, query_texts, k=5):\n",
        "    \"\"\"Benchmark search performance of a FAISS index\"\"\"\n",
        "    times = []\n",
        "    \n",
        "    for query in query_texts:\n",
        "        query_embedding = model.encode([query])\n",
        "        \n",
        "        start_time = time.time()\n",
        "        distances, indices = index.search(query_embedding.astype('float32'), k)\n",
        "        end_time = time.time()\n",
        "        \n",
        "        times.append(end_time - start_time)\n",
        "    \n",
        "    return {\n",
        "        'avg_time': np.mean(times) * 1000,  # Convert to milliseconds\n",
        "        'min_time': np.min(times) * 1000,\n",
        "        'max_time': np.max(times) * 1000,\n",
        "        'std_time': np.std(times) * 1000\n",
        "    }\n",
        "\n",
        "# Create test queries for benchmarking\n",
        "benchmark_queries = [\n",
        "    \"Patient registration and demographics\",\n",
        "    \"Blood pressure monitoring\",\n",
        "    \"GDPR compliance data export\",\n",
        "    \"Emergency triage workflow\",\n",
        "    \"Medication allergy alerts\"\n",
        "]\n",
        "\n",
        "# Benchmark each index type\n",
        "print(\"üèÉ Performance Benchmarking Results\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "indices_to_test = {\n",
        "    'Flat (L2)': index_flat,\n",
        "    'IVF': index_ivf,\n",
        "    'HNSW': index_hnsw\n",
        "}\n",
        "\n",
        "# Set search parameters for IVF index\n",
        "index_ivf.nprobe = 3  # Number of clusters to search\n",
        "\n",
        "benchmark_results = {}\n",
        "for name, index in indices_to_test.items():\n",
        "    results = benchmark_index(index, benchmark_queries)\n",
        "    benchmark_results[name] = results\n",
        "    \n",
        "    print(f\"\\n{name} Index:\")\n",
        "    print(f\"  Average search time: {results['avg_time']:.2f} ms\")\n",
        "    print(f\"  Min/Max: {results['min_time']:.2f} ms / {results['max_time']:.2f} ms\")\n",
        "    print(f\"  Std deviation: {results['std_time']:.2f} ms\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "benchmark_df = pd.DataFrame(benchmark_results).T\n",
        "benchmark_df = benchmark_df.round(2)\n",
        "print(\"\\nüìä Summary Comparison:\")\n",
        "print(benchmark_df)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 8: Save and Load Index\n",
        "\n",
        "For production use, we need to save and load our FAISS indices:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Index saved to: ../data/faiss_indices/healthcare_test_cases.index\n",
            "‚úÖ Metadata saved to: ../data/faiss_indices/test_cases_metadata.csv\n",
            "‚úÖ Configuration saved to: ../data/faiss_indices/index_config.json\n",
            "\n",
            "üìÅ Total size of saved files:\n",
            "  healthcare_test_cases.index: 22.54 KB\n",
            "  test_cases_metadata.csv: 11.99 KB\n",
            "  index_config.json: 0.17 KB\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Create directory for storing indices\n",
        "index_dir = \"../data/faiss_indices\"\n",
        "os.makedirs(index_dir, exist_ok=True)\n",
        "\n",
        "# Save the FAISS index\n",
        "index_path = os.path.join(index_dir, \"healthcare_test_cases.index\")\n",
        "faiss.write_index(index_flat, index_path)\n",
        "print(f\"‚úÖ Index saved to: {index_path}\")\n",
        "\n",
        "# Save the metadata (test cases DataFrame)\n",
        "metadata_path = os.path.join(index_dir, \"test_cases_metadata.csv\")\n",
        "df_test_cases.to_csv(metadata_path, index=False)\n",
        "print(f\"‚úÖ Metadata saved to: {metadata_path}\")\n",
        "\n",
        "# Save index configuration\n",
        "config = {\n",
        "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
        "    \"embedding_dimension\": dimension,\n",
        "    \"num_test_cases\": len(df_test_cases),\n",
        "    \"index_type\": \"Flat L2\",\n",
        "    \"created_date\": pd.Timestamp.now().isoformat()\n",
        "}\n",
        "\n",
        "config_path = os.path.join(index_dir, \"index_config.json\")\n",
        "with open(config_path, 'w') as f:\n",
        "    json.dump(config, f, indent=2)\n",
        "print(f\"‚úÖ Configuration saved to: {config_path}\")\n",
        "\n",
        "print(f\"\\nüìÅ Total size of saved files:\")\n",
        "for file in [index_path, metadata_path, config_path]:\n",
        "    if os.path.exists(file):\n",
        "        size = os.path.getsize(file) / 1024  # Convert to KB\n",
        "        print(f\"  {os.path.basename(file)}: {size:.2f} KB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 9: Load and Verify Saved Index\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Index loaded from: ../data/faiss_indices/healthcare_test_cases.index\n",
            "   Number of vectors: 15\n",
            "‚úÖ Metadata loaded: 15 test cases\n",
            "‚úÖ Configuration loaded:\n",
            "   embedding_model: all-MiniLM-L6-v2\n",
            "   embedding_dimension: 384\n",
            "   num_test_cases: 15\n",
            "   index_type: Flat L2\n",
            "   created_date: 2025-09-13T17:10:37.808830\n",
            "\n",
            "üß™ Testing loaded index with query: 'Patient data HIPAA compliance'\n",
            "\n",
            "Search results from loaded index:\n",
            " rank    id                                           title           category priority  similarity_score\n",
            "    1 TC001 Patient Registration with Complete Demographics Patient Management     High          0.519909\n",
            "    2 TC014                 Patient Portal Secure Messaging Patient Engagement   Medium          0.470349\n",
            "    3 TC006                        GDPR Data Export Request         Compliance     High          0.465592\n"
          ]
        }
      ],
      "source": [
        "# Load the saved index\n",
        "loaded_index = faiss.read_index(index_path)\n",
        "print(f\"‚úÖ Index loaded from: {index_path}\")\n",
        "print(f\"   Number of vectors: {loaded_index.ntotal}\")\n",
        "\n",
        "# Load the metadata\n",
        "loaded_df = pd.read_csv(metadata_path)\n",
        "# Convert compliance column back to list\n",
        "loaded_df['compliance'] = loaded_df['compliance'].apply(lambda x: eval(x) if pd.notna(x) else [])\n",
        "print(f\"‚úÖ Metadata loaded: {len(loaded_df)} test cases\")\n",
        "\n",
        "# Load configuration\n",
        "with open(config_path, 'r') as f:\n",
        "    loaded_config = json.load(f)\n",
        "print(f\"‚úÖ Configuration loaded:\")\n",
        "for key, value in loaded_config.items():\n",
        "    print(f\"   {key}: {value}\")\n",
        "\n",
        "# Verify the loaded index works correctly\n",
        "test_query = \"Patient data HIPAA compliance\"\n",
        "print(f\"\\nüß™ Testing loaded index with query: '{test_query}'\")\n",
        "\n",
        "results = search_similar_test_cases(test_query, loaded_index, k=3, df=loaded_df)\n",
        "print(\"\\nSearch results from loaded index:\")\n",
        "print(results.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 10: Practical Applications for Test Case Generation\n",
        "\n",
        "Based on the NASSCOM problem statement, here are practical applications of this FAISS vector database:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXAMPLE 1: Generate Test Case Template\n",
            "================================================================================\n",
            "\n",
            "üìù New Requirement: Verify that the system can handle multiple concurrent telemedicine sessions without performance degradation\n",
            "\n",
            "ü§ñ Generated Test Case Template:\n",
            "  id: TC_NEW\n",
            "  title: [Generated] Verify that the system can handle multiple concurr...\n",
            "  description: Test case for: Verify that the system can handle multiple concurrent telemedicine sessions without performance degradation\n",
            "  category: Telehealth\n",
            "  priority: High\n",
            "  compliance: ['HIPAA', 'State Licensure']\n",
            "  test_steps: 1. [Step based on requirement]\n",
            "2. [Verification step]\n",
            "3. [Validation step]\n",
            "  expected_result: [Expected outcome based on requirement]\n",
            "\n",
            "  Based on similar tests: ['TC009', 'TC014', 'TC006']\n",
            "  Similarity scores: ['0.425', '0.395', '0.389']\n"
          ]
        }
      ],
      "source": [
        "class TestCaseGenerator:\n",
        "    \"\"\"\n",
        "    A class that uses FAISS vector search to help generate new test cases\n",
        "    based on requirements and existing test cases\n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, index, df_test_cases, model):\n",
        "        self.index = index\n",
        "        self.df_test_cases = df_test_cases\n",
        "        self.model = model\n",
        "    \n",
        "    def find_similar_tests(self, requirement: str, k: int = 5):\n",
        "        \"\"\"Find existing test cases similar to a new requirement\"\"\"\n",
        "        return search_similar_test_cases(requirement, self.index, k, self.df_test_cases)\n",
        "    \n",
        "    def generate_test_template(self, requirement: str):\n",
        "        \"\"\"\n",
        "        Generate a test case template based on similar existing tests\n",
        "        This simulates what an LLM would do with the retrieved context\n",
        "        \"\"\"\n",
        "        # Find similar test cases\n",
        "        similar_tests = self.find_similar_tests(requirement, k=3)\n",
        "        \n",
        "        if len(similar_tests) == 0:\n",
        "            return None\n",
        "        \n",
        "        # Extract common patterns from similar tests\n",
        "        top_test = self.df_test_cases[self.df_test_cases['id'] == similar_tests.iloc[0]['id']].iloc[0]\n",
        "        \n",
        "        # Create template based on the most similar test\n",
        "        template = {\n",
        "            \"id\": \"TC_NEW\",\n",
        "            \"title\": f\"[Generated] {requirement[:50]}...\",\n",
        "            \"description\": f\"Test case for: {requirement}\",\n",
        "            \"category\": top_test['category'],\n",
        "            \"priority\": top_test['priority'],\n",
        "            \"compliance\": top_test['compliance'],\n",
        "            \"test_steps\": \"1. [Step based on requirement]\\n2. [Verification step]\\n3. [Validation step]\",\n",
        "            \"expected_result\": \"[Expected outcome based on requirement]\",\n",
        "            \"based_on\": similar_tests['id'].tolist(),\n",
        "            \"similarity_scores\": similar_tests['similarity_score'].tolist()\n",
        "        }\n",
        "        \n",
        "        return template\n",
        "    \n",
        "    def identify_gaps(self, requirements: List[str], threshold: float = 0.7):\n",
        "        \"\"\"\n",
        "        Identify requirements that don't have similar test cases\n",
        "        (potential gaps in test coverage)\n",
        "        \"\"\"\n",
        "        gaps = []\n",
        "        \n",
        "        for req in requirements:\n",
        "            results = self.find_similar_tests(req, k=1)\n",
        "            \n",
        "            if len(results) == 0 or results.iloc[0]['similarity_score'] < threshold:\n",
        "                gaps.append({\n",
        "                    'requirement': req,\n",
        "                    'best_match_score': results.iloc[0]['similarity_score'] if len(results) > 0 else 0,\n",
        "                    'best_match_id': results.iloc[0]['id'] if len(results) > 0 else None\n",
        "                })\n",
        "        \n",
        "        return gaps\n",
        "\n",
        "# Initialize the generator\n",
        "generator = TestCaseGenerator(index_flat, df_test_cases, model)\n",
        "\n",
        "# Example 1: Generate test case template for a new requirement\n",
        "print(\"=\" * 80)\n",
        "print(\"EXAMPLE 1: Generate Test Case Template\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "new_requirement = \"Verify that the system can handle multiple concurrent telemedicine sessions without performance degradation\"\n",
        "template = generator.generate_test_template(new_requirement)\n",
        "\n",
        "if template:\n",
        "    print(f\"\\nüìù New Requirement: {new_requirement}\")\n",
        "    print(\"\\nü§ñ Generated Test Case Template:\")\n",
        "    for key, value in template.items():\n",
        "        if key not in ['based_on', 'similarity_scores']:\n",
        "            print(f\"  {key}: {value}\")\n",
        "    print(f\"\\n  Based on similar tests: {template['based_on']}\")\n",
        "    print(f\"  Similarity scores: {[f'{s:.3f}' for s in template['similarity_scores']]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "EXAMPLE 2: Identify Test Coverage Gaps\n",
            "================================================================================\n",
            "\n",
            "üîç Analyzing requirements for test coverage gaps...\n",
            "\n",
            "‚ö†Ô∏è  Found 5 potential gaps in test coverage:\n",
            "\n",
            "  ‚ùå Requirement: Blockchain-based audit trail for all transactions...\n",
            "     Best match score: 0.428\n",
            "     Closest test case: TC012\n",
            "\n",
            "  ‚ùå Requirement: Patient allergy checking system...\n",
            "     Best match score: 0.517\n",
            "     Closest test case: TC003\n",
            "\n",
            "  ‚ùå Requirement: AI-powered diagnosis suggestions...\n",
            "     Best match score: 0.443\n",
            "     Closest test case: TC013\n",
            "\n",
            "  ‚ùå Requirement: GDPR compliance for EU patients...\n",
            "     Best match score: 0.498\n",
            "     Closest test case: TC006\n",
            "\n",
            "  ‚ùå Requirement: Quantum encryption for data security...\n",
            "     Best match score: 0.385\n",
            "     Closest test case: TC014\n",
            "\n",
            "üìä Test Coverage Summary:\n",
            "   Total requirements: 5\n",
            "   Covered: 0\n",
            "   Gaps: 5\n",
            "   Coverage: 0.0%\n"
          ]
        }
      ],
      "source": [
        "# Example 2: Identify gaps in test coverage\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"EXAMPLE 2: Identify Test Coverage Gaps\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "new_requirements = [\n",
        "    \"Blockchain-based audit trail for all transactions\", \n",
        "    \"Patient allergy checking system\", \n",
        "    \"AI-powered diagnosis suggestions\", \n",
        "    \"GDPR compliance for EU patients\",  \n",
        "    \"Quantum encryption for data security\"  \n",
        "]\n",
        "\n",
        "print(\"\\nüîç Analyzing requirements for test coverage gaps...\")\n",
        "gaps = generator.identify_gaps(new_requirements, threshold=0.6)\n",
        "\n",
        "if gaps:\n",
        "    print(f\"\\n‚ö†Ô∏è  Found {len(gaps)} potential gaps in test coverage:\\n\")\n",
        "    for gap in gaps:\n",
        "        print(f\"  ‚ùå Requirement: {gap['requirement'][:60]}...\")\n",
        "        print(f\"     Best match score: {gap['best_match_score']:.3f}\")\n",
        "        if gap['best_match_id']:\n",
        "            print(f\"     Closest test case: {gap['best_match_id']}\")\n",
        "        print()\n",
        "else:\n",
        "    print(\"\\n‚úÖ All requirements have adequate test coverage!\")\n",
        "\n",
        "# Show coverage summary\n",
        "covered = len(new_requirements) - len(gaps)\n",
        "coverage_percentage = (covered / len(new_requirements)) * 100\n",
        "print(f\"üìä Test Coverage Summary:\")\n",
        "print(f\"   Total requirements: {len(new_requirements)}\")\n",
        "print(f\"   Covered: {covered}\")\n",
        "print(f\"   Gaps: {len(gaps)}\")\n",
        "print(f\"   Coverage: {coverage_percentage:.1f}%\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
